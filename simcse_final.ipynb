{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526b4798",
   "metadata": {},
   "source": [
    "## objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc1b04",
   "metadata": {},
   "source": [
    "語意檢索：檢索系統不再拘泥於用戶詢問的字面本身，而是能精確捕捉到他後面的意圖並以此來搜索最好最精確的結果。\n",
    "\n",
    "對比訓練：減少正樣本的距離，增加負樣本的距離\n",
    "\n",
    "好的對比系統滿足：\n",
    "\n",
    "-Alignment：相似的例子應該有接近的特徵，距離比較近\n",
    "\n",
    "-Uniformity：系統應該傾向在特徵裡保留盡可能多的訊息。特徵分布越均勻代表兩倆訊息有差異，保留的訊息越充分。\n",
    "\n",
    "在SimCSE上進行無監督訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a103fc",
   "metadata": {},
   "source": [
    "## experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e8e442",
   "metadata": {},
   "source": [
    "方法：把同樣的input放進encoder兩次，透過不同的dropout取得不同的embedding，使得語意上有不同的差異\n",
    "\n",
    "處理偽樣本問題：增大batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85ec4f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2bb4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import abc\n",
    "import functools\n",
    "from functools import partial\n",
    "from visualdl import LogWriter\n",
    "import paddle\n",
    "import paddlenlp\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup, AutoModel, AutoTokenizer\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorWithPadding\n",
    "from paddlenlp.datasets import DatasetBuilder\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "from paddle.dataset.common import md5file\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf34ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/u9285752')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba08687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#offending lines skipped (Lines with too many fields (e.g. a csv line with too many commas))\n",
    "df = pd.read_csv(\"./國泰/train.tsv\", delimiter=\"\\t\",on_bad_lines='skip', header=None, index_col=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06fb5d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>用微信都6年，微信没有微粒贷功能</td>\n",
       "      <td>4。号码来微粒贷</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>微信消费算吗</td>\n",
       "      <td>还有多少钱没还</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>交易密码忘记了找回密码绑定的手机卡也掉了</td>\n",
       "      <td>怎么最近安全老是要改密码呢好麻烦</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>你好我昨天晚上申请的没有打电话给我今天之内一定会打吗？</td>\n",
       "      <td>什么时候可以到账</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“微粒贷开通\"</td>\n",
       "      <td>你好，我的微粒贷怎么没有开通呢</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0                 1    2\n",
       "0             用微信都6年，微信没有微粒贷功能          4。号码来微粒贷  0.0\n",
       "1                       微信消费算吗           还有多少钱没还  0.0\n",
       "2         交易密码忘记了找回密码绑定的手机卡也掉了  怎么最近安全老是要改密码呢好麻烦  0.0\n",
       "3  你好我昨天晚上申请的没有打电话给我今天之内一定会打吗？          什么时候可以到账  0.0\n",
       "4                      “微粒贷开通\"   你好，我的微粒贷怎么没有开通呢  0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27df7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"text_a\", \"text_b\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "330e6ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86196"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc87f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "strange_data = []\n",
    "for idx,(i,j) in enumerate(zip(df['text_a'], df['text_b'])):\n",
    "    if len(i.split('\\t')) > 1 or len(j.split('\\t')) > 1:\n",
    "        strange_data.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc52036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(labels=strange_data,axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f34e8c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86185"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc919977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    86185.000000\n",
      "mean        23.726774\n",
      "std         11.283784\n",
      "min          3.000000\n",
      "25%         16.000000\n",
      "50%         21.000000\n",
      "75%         28.000000\n",
      "max        201.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# count the length of queries\n",
    "print((df['text_a']+df['text_b']).map(len).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "545e6f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,val_data = train_test_split(df,train_size=0.9,random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab9a524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['text_b', 'label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c0cc2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.reset_index(drop=True)\n",
    "val_data=val_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1e9d4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['text_b']\n",
    "corpus.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "77419312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>不被邀请就不能借？</td>\n",
       "      <td>开通了微粒贷，可以贷多少？</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>为什么我把版本更新了还没看到微粒</td>\n",
       "      <td>怎么我微信红包没微粒代呢？</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>我钱包里没有这个选项</td>\n",
       "      <td>怎么进入借贷页面</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>能把还款日推迟一日吗</td>\n",
       "      <td>可以更改还款日么？</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>为什么还没有</td>\n",
       "      <td>回访的电话怎么还没打来</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             text_a         text_b  label\n",
       "0         不被邀请就不能借？  开通了微粒贷，可以贷多少？    0.0\n",
       "1  为什么我把版本更新了还没看到微粒  怎么我微信红包没微粒代呢？    1.0\n",
       "2        我钱包里没有这个选项       怎么进入借贷页面    1.0\n",
       "3        能把还款日推迟一日吗      可以更改还款日么？    1.0\n",
       "4            为什么还没有    回访的电话怎么还没打来    0.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f0d1cd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>如何提前分期还款</td>\n",
       "      <td>4万我如果借款20天一共要还款是多少？</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>两天为什么还没有接到确认电话</td>\n",
       "      <td>QQ申请了他说等待电话确认要多久都过了两天了</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>你好，请问借款可以用于信用卡还款吗？</td>\n",
       "      <td>不能用于购房还有证劵投资还有呢</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>为什么我的微信没有微粒贷</td>\n",
       "      <td>4。号码来微粒贷</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>刚才没有接到你们的审核电话</td>\n",
       "      <td>审批周期多久？</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               text_a                  text_b  label\n",
       "0            如何提前分期还款     4万我如果借款20天一共要还款是多少？    0.0\n",
       "1      两天为什么还没有接到确认电话  QQ申请了他说等待电话确认要多久都过了两天了    1.0\n",
       "2  你好，请问借款可以用于信用卡还款吗？         不能用于购房还有证劵投资还有呢    0.0\n",
       "3        为什么我的微信没有微粒贷                4。号码来微粒贷    0.0\n",
       "4       刚才没有接到你们的审核电话                 审批周期多久？    0.0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bbecee5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            4。号码来微粒贷\n",
       "1             还有多少钱没还\n",
       "2    怎么最近安全老是要改密码呢好麻烦\n",
       "3            什么时候可以到账\n",
       "4     你好，我的微粒贷怎么没有开通呢\n",
       "Name: text_b, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "592f3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_no_duplicate = []\n",
    "for i in list(corpus):\n",
    "    if i not in list(val_data['text_a']):\n",
    "        corpus_no_duplicate.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "87df55f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20490"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "88531c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14969"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_no_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "070c9187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4。号码来微粒贷</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>还有多少钱没还</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>怎么最近安全老是要改密码呢好麻烦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>什么时候可以到账</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>你好，我的微粒贷怎么没有开通呢</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "0          4。号码来微粒贷\n",
       "1           还有多少钱没还\n",
       "2  怎么最近安全老是要改密码呢好麻烦\n",
       "3          什么时候可以到账\n",
       "4   你好，我的微粒贷怎么没有开通呢"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_no_duplicate = pd.DataFrame(corpus_no_duplicate)\n",
    "corpus_no_duplicate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0f185178",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_nolabel = val_data.drop(['label'], axis=1)\n",
    "val_data_texta = val_data_nolabel.drop(['text_b'], axis=1) # validation\n",
    "val_data_textb = val_data_nolabel.drop(['text_a'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f683aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (val_data['label']==1.0)\n",
    "val_pair = val_data_nolabel.loc[filt] #count recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e1b2632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"./國泰/train.csv\", header=None, index=False)\n",
    "val_data.to_csv(\"./國泰/val.csv\",header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d84ec891",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pair.to_csv(\"./國泰/val_pair.csv\", header=None, index=False)\n",
    "val_data_texta.to_csv(\"./國泰/val_texta.csv\", header=None, index=False)\n",
    "val_data_textb.to_csv(\"./國泰/val_textb.csv\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b8dd5271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus.to_csv(\"./國泰/corpus.csv\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47e67a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_no_duplicate.to_csv(\"./corpus.csv\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c18a8d0",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204eb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_simcse_text(data_path):\n",
    "    \"\"\"Reads data.\"\"\"\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = line.rstrip()\n",
    "            yield {'text_a': data, 'text_b': data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fee868e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example(example, tokenizer, max_seq_length=512):\n",
    "    \"\"\"\n",
    "        max_seq_len(obj:`int`): The maximum total input sequence length after tokenization. \n",
    "            Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "    Returns:\n",
    "        input_ids(obj:`list[int]`): The list of query token ids.\n",
    "        token_type_ids(obj: `list[int]`): List of query sequence pair mask.\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "    for key, text in example.items():\n",
    "\n",
    "        # do_train\n",
    "        encoded_inputs = tokenizer(text=text, max_seq_len=max_seq_length)\n",
    "        input_ids = encoded_inputs[\"input_ids\"]\n",
    "        token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "        result += [input_ids, token_type_ids]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12013914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(dataset,\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(dataset=dataset,\n",
    "                                batch_sampler=batch_sampler,\n",
    "                                collate_fn=batchify_fn,\n",
    "                                return_list=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91b78fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCSE(nn.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained_model,\n",
    "                 dropout=None,\n",
    "                 margin=0.0,\n",
    "                 scale=20,\n",
    "                 output_emb_size=None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.ptm = pretrained_model\n",
    "        self.dropout = nn.Dropout(dropout if dropout is not None else 0.1)\n",
    "\n",
    "        self.output_emb_size = output_emb_size\n",
    "        if output_emb_size > 0:\n",
    "            weight_attr = paddle.ParamAttr(\n",
    "                initializer=paddle.nn.initializer.TruncatedNormal(std=0.02)) #随机截断正態分布初始化函数\n",
    "            '''\n",
    "            # 生成的隨機值與RandomNormal類似，但是在距離平均值两兩個標準差之外的随机值會被丟棄並重新生成\n",
    "            # 讓輸入的值盡量落在梯度下降明顯的區間範圍內（activation function中）縮短訓練時間，幫助收斂\n",
    "            # 如果太小如果權重值小，會導致在反向傳播時得到很小的梯度值，引起梯度消失\n",
    "            '''\n",
    "            self.emb_reduce_linear = paddle.nn.Linear(768,\n",
    "                                                      output_emb_size,\n",
    "                                                      weight_attr=weight_attr)\n",
    "\n",
    "        self.sacle = scale\n",
    "        '''\n",
    "        # 加入jit注釋能夠把該提取向量的函數導出成靜態圖\n",
    "        # 靜態圖在部署方面更具有性能的優勢。靜態圖程序在編譯執行時，先搭建模型再對網絡進行計算操作\n",
    "        # 預先搭建好的網絡可以在其他程式語言被重新解析執行，而且擁有整理網絡結構也能進行一些網絡結構的優化\n",
    "        # input_id,token_type_id\n",
    "        '''\n",
    "    @paddle.jit.to_static(input_spec=[\n",
    "        paddle.static.InputSpec(shape=[None, None], dtype='int64'),\n",
    "        paddle.static.InputSpec(shape=[None, None], dtype='int64')])\n",
    "    \n",
    "    def get_pooled_embedding(self,\n",
    "                             input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             position_ids=None,\n",
    "                             attention_mask=None):\n",
    "\n",
    "        # map sentence to a vector\n",
    "        sequence_output, cls_embedding = self.ptm(input_ids, token_type_ids,\n",
    "                                                  position_ids, attention_mask)\n",
    "\n",
    "          \n",
    "       # output_emb_size=256\n",
    "       # the larger the number, the richer the semantic information, but the more resources are consumed\n",
    "        \n",
    "        if self.output_emb_size > 0:\n",
    "            cls_embedding = self.emb_reduce_linear(cls_embedding)\n",
    "\n",
    "        cls_embedding = self.dropout(cls_embedding)\n",
    "        cls_embedding = F.normalize(cls_embedding, axis=-1)\n",
    "\n",
    "        return cls_embedding\n",
    "\n",
    "    def get_semantic_embedding(self, data_loader):\n",
    "        self.eval()\n",
    "        with paddle.no_grad():\n",
    "            for batch_data in data_loader:\n",
    "                input_ids, token_type_ids = batch_data\n",
    "                input_ids = paddle.to_tensor(input_ids)\n",
    "                token_type_ids = paddle.to_tensor(token_type_ids)\n",
    "\n",
    "                text_embeddings = self.get_pooled_embedding(input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "                yield text_embeddings\n",
    "\n",
    "    def cosine_sim(self,\n",
    "                   query_input_ids,\n",
    "                   title_input_ids,\n",
    "                   query_token_type_ids=None,\n",
    "                   query_position_ids=None,\n",
    "                   query_attention_mask=None,\n",
    "                   title_token_type_ids=None,\n",
    "                   title_position_ids=None,\n",
    "                   title_attention_mask=None):\n",
    "\n",
    "        query_cls_embedding = self.get_pooled_embedding(query_input_ids,\n",
    "                                                        query_token_type_ids,\n",
    "                                                        query_position_ids,\n",
    "                                                        query_attention_mask)\n",
    "\n",
    "        title_cls_embedding = self.get_pooled_embedding(title_input_ids,\n",
    "                                                        title_token_type_ids,\n",
    "                                                        title_position_ids,\n",
    "                                                        title_attention_mask)\n",
    "\n",
    "        \n",
    "        cosine_sim = paddle.sum(query_cls_embedding * title_cls_embedding, axis=-1)\n",
    "\n",
    "        return cosine_sim\n",
    "\n",
    "    def forward(self,\n",
    "                query_input_ids,\n",
    "                title_input_ids,\n",
    "                query_token_type_ids=None,\n",
    "                query_position_ids=None,\n",
    "                query_attention_mask=None,\n",
    "                title_token_type_ids=None,\n",
    "                title_position_ids=None,\n",
    "                title_attention_mask=None):\n",
    "\n",
    "        # 1st encoding\n",
    "        query_cls_embedding = self.get_pooled_embedding(query_input_ids,\n",
    "                                                        query_token_type_ids,\n",
    "                                                        query_position_ids,\n",
    "                                                        query_attention_mask) #shape=[batch_size, 256]\n",
    " \n",
    "        # 2nd encoding\n",
    "        title_cls_embedding = self.get_pooled_embedding(title_input_ids,\n",
    "                                                        title_token_type_ids,\n",
    "                                                        title_position_ids,\n",
    "                                                        title_attention_mask)\n",
    "        \n",
    "        # similarity matrix, shape = [batch_size, batch_size]\n",
    "        cosine_sim = paddle.matmul(query_cls_embedding,\n",
    "                                   title_cls_embedding,\n",
    "                                   transpose_y=True) # matmul：矩陣相乘  \n",
    "        \n",
    "\n",
    "# adjust the degree of attention to difficult samples: \n",
    "# the smaller the temperature coefficient, the more attention is paid to separating this sample from the most similar difficult samples to obtain a more uniform representation \n",
    "# However, difficult samples are often similar to this sample, and many difficult negative samples are actually potential positive samples. \n",
    "# Excessive forcing to separate from difficult samples will destroy the learned latent semantic structure. \n",
    "# 當溫度係數很小時，越相似的負例對整體loss影響越大\n",
    "\n",
    "        cosine_sim *= self.sacle\n",
    "\n",
    "        labels = paddle.arange(0, query_cls_embedding.shape[0], dtype='int64') #[0,1,2,3,...,batch_size-1]\n",
    "        labels = paddle.reshape(labels, shape=[-1, 1]) #[[0],[1],[2],[3]...]\n",
    "\n",
    "\n",
    "        # Converted to classification task: diagonal - positive examples, rest - negative examples\n",
    "        loss = F.cross_entropy(input=cosine_sim, label=labels)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ad24dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-02 00:31:39,877] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieModel'> to load 'ernie-3.0-base-zh'.\u001b[0m\n",
      "\u001b[32m[2022-08-02 00:31:39,878] [    INFO]\u001b[0m - Already cached /home/u9285752/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams\u001b[0m\n",
      "W0802 00:31:39.880066    95 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.7, Runtime API Version: 10.2\n",
      "W0802 00:31:39.881101    95 gpu_resources.cc:91] device: 0, cuDNN Version: 8.4.\n",
      "\u001b[32m[2022-08-02 00:31:47,901] [    INFO]\u001b[0m - Weights from pretrained model not used in ErnieModel: ['cls.predictions.transform.weight', 'cls.predictions.layer_norm.weight', 'cls.predictions.transform.bias', 'cls.predictions.layer_norm.bias', 'cls.predictions.decoder_bias']\u001b[0m\n",
      "\u001b[32m[2022-08-02 00:31:49,771] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-base-zh'.\u001b[0m\n",
      "\u001b[32m[2022-08-02 00:31:49,773] [    INFO]\u001b[0m - Already cached /home/u9285752/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt\u001b[0m\n",
      "\u001b[32m[2022-08-02 00:31:49,817] [    INFO]\u001b[0m - tokenizer config file saved in /home/u9285752/.paddlenlp/models/ernie-3.0-base-zh/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-08-02 00:31:49,820] [    INFO]\u001b[0m - Special tokens file saved in /home/u9285752/.paddlenlp/models/ernie-3.0-base-zh/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from ernie-3.0-base-zh\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ernie-3.0-base-zh\"\n",
    "pretrained_model = AutoModel.from_pretrained(model_name)\n",
    "print(\"loading model from {}\".format(model_name))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46dd8ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1 train_loss:  0.14876 val_loss:  0.00488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-02 00:39:26,096] [    INFO]\u001b[0m - tokenizer config file saved in ./國泰/train_3/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-08-02 00:39:26,098] [    INFO]\u001b[0m - Special tokens file saved in ./國泰/train_3/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2 train_loss:  0.02036 val_loss:  0.00458\n",
      "epoch:  3 train_loss:  0.01645 val_loss:  0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-02 00:45:51,868] [    INFO]\u001b[0m - tokenizer config file saved in ./國泰/train_3/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-08-02 00:45:51,869] [    INFO]\u001b[0m - Special tokens file saved in ./國泰/train_3/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  4 train_loss:  0.01323 val_loss:  0.00449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-02 00:52:08,868] [    INFO]\u001b[0m - tokenizer config file saved in ./國泰/train_3/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-08-02 00:52:08,869] [    INFO]\u001b[0m - Special tokens file saved in ./國泰/train_3/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5 train_loss:  0.01452 val_loss:  0.00453\n",
      "epoch:  6 train_loss:  0.01128 val_loss:  0.00447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-02 00:58:34,949] [    INFO]\u001b[0m - tokenizer config file saved in ./國泰/train_3/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-08-02 00:58:34,950] [    INFO]\u001b[0m - Special tokens file saved in ./國泰/train_3/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  7 train_loss:  0.01224 val_loss:  0.00456\n",
      "epoch:  8 train_loss:  0.01171 val_loss:  0.00449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-02 01:04:59,999] [    INFO]\u001b[0m - tokenizer config file saved in ./國泰/train_3/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-08-02 01:05:00,000] [    INFO]\u001b[0m - Special tokens file saved in ./國泰/train_3/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  9 train_loss:  0.01129 val_loss:  0.00438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-02 01:11:17,109] [    INFO]\u001b[0m - tokenizer config file saved in ./國泰/train_3/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-08-02 01:11:17,110] [    INFO]\u001b[0m - Special tokens file saved in ./國泰/train_3/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  10 train_loss:  0.01044 val_loss:  0.00444\n",
      "epoch:  11 train_loss:  0.01029 val_loss:  0.00444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-02 01:17:42,366] [    INFO]\u001b[0m - tokenizer config file saved in ./國泰/train_3/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-08-02 01:17:42,368] [    INFO]\u001b[0m - Special tokens file saved in ./國泰/train_3/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  12 train_loss:  0.01059 val_loss:  0.00447\n",
      "epoch:  13 train_loss:  0.00988 val_loss:  0.00448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-02 01:24:07,499] [    INFO]\u001b[0m - tokenizer config file saved in ./國泰/train_3/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-08-02 01:24:07,500] [    INFO]\u001b[0m - Special tokens file saved in ./國泰/train_3/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  14 train_loss:  0.00897 val_loss:  0.00449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-02 01:30:24,163] [    INFO]\u001b[0m - tokenizer config file saved in ./國泰/train_3/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-08-02 01:30:24,164] [    INFO]\u001b[0m - Special tokens file saved in ./國泰/train_3/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  15 train_loss:  0.00932 val_loss:  0.00442\n",
      "epoch:  16 train_loss:  0.0096 val_loss:  0.00443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-02 01:36:49,129] [    INFO]\u001b[0m - tokenizer config file saved in ./國泰/train_3/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-08-02 01:36:49,130] [    INFO]\u001b[0m - Special tokens file saved in ./國泰/train_3/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  17 train_loss:  0.00863 val_loss:  0.00444\n",
      "epoch:  18 train_loss:  0.00836 val_loss:  0.00443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-02 01:43:14,010] [    INFO]\u001b[0m - tokenizer config file saved in ./國泰/train_3/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-08-02 01:43:14,012] [    INFO]\u001b[0m - Special tokens file saved in ./國泰/train_3/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  19 train_loss:  0.00834 val_loss:  0.00441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-02 01:49:30,290] [    INFO]\u001b[0m - tokenizer config file saved in ./國泰/train_3/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-08-02 01:49:30,291] [    INFO]\u001b[0m - Special tokens file saved in ./國泰/train_3/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  20 train_loss:  0.00889 val_loss:  0.00441\n",
      "totally cost 4638.131017208099\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "\n",
    "    writer=LogWriter(logdir=\"./國泰/writer\")\n",
    "    train_ds = load_dataset(read_simcse_text, data_path=\"./國泰/train.csv\", lazy=False)\n",
    "    dev_ds = load_dataset(read_simcse_text, data_path=\"./國泰/val_texta.csv\", lazy=False)\n",
    "\n",
    "    #Partial allow one to derive a function with x parameters to a function with fewer parameters and fixed values set for the more limited function.\n",
    "    trans_func = partial(convert_example, tokenizer=tokenizer, max_seq_length=30) # convert example to feature\n",
    "    #trans_func_eval = partial(convert_example_dev, tokenizer=tokenizer, max_seq_length=30)\n",
    "    #Pad: Padding multiple sentences with different lengths to a uniform length, and take the maximum length of the N input data\n",
    "    #Tuple: wraps multiple batchify functions together\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype=\"int64\"),  # query_input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype=\"int64\"),  # query_segment\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype=\"int64\"),  # title_input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype=\"int64\"),  # tilte_segment\n",
    "    ): [data for data in fn(samples)]\n",
    "\n",
    "        \n",
    "    train_data_loader = create_dataloader(train_ds, mode='train', batch_size=128, batchify_fn=batchify_fn, trans_fn=trans_func)\n",
    "    dev_data_loader = create_dataloader(dev_ds, mode='dev', batch_size=64, batchify_fn=batchify_fn, trans_fn=trans_func)\n",
    "    \n",
    "    scale = 20 \n",
    "    output_emb_size = 256 \n",
    "    model = SimCSE(pretrained_model, scale=scale, output_emb_size=output_emb_size)\n",
    "\n",
    "    init_from_ckpt = None\n",
    "    if init_from_ckpt and os.path.isfile(init_from_ckpt):\n",
    "        state_dict = paddle.load(init_from_ckpt)\n",
    "        model.set_dict(state_dict)\n",
    "        print(\"warmup from:{}\".format(init_from_ckpt))\n",
    "\n",
    "    epochs = 20\n",
    "    num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "    learning_rate = 1e-4\n",
    "    \n",
    "    \n",
    "    # increases learning rate linearly from 0 to given learning_rate, \n",
    "    # after this warmup period learning rate would be decreased linearly from the base learning rate to 0\n",
    "    warmup_proportion = 0.0 # Linear warmup proption over the training process\n",
    "    lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "\n",
    "    # All bias and LayerNorm parameters are excluded.\n",
    "    # applying weight decay to the bias units usually makes only a small difference to the final network\n",
    "    # norm parameters are meant to scale and shift the normalized input of the layer. As such, forcing these values to a lower value would affect the distribution and result in inferior results\n",
    "    decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])]\n",
    "    optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay= 0.01,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "    time_start=time.time()\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    \n",
    "    save_steps = 1000\n",
    "    eval_step = 2\n",
    "    save_dir = \"./國泰/train_3\"\n",
    "    loss_base = 9999\n",
    "    train_curve = []\n",
    "    val_curve = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = 0.0\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "            # query_input = title_input\n",
    "            query_input_ids, query_token_type_ids, title_input_ids, title_token_type_ids = batch\n",
    "            loss = model(\n",
    "                query_input_ids=query_input_ids,\n",
    "                title_input_ids=title_input_ids,\n",
    "                query_token_type_ids=query_token_type_ids,\n",
    "                title_token_type_ids=title_token_type_ids)\n",
    "            global_step += 1\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            optimizer.clear_grad()\n",
    "            if global_step % save_steps == 0:\n",
    "                save_path = os.path.join(save_dir, \"model_%d\" % (global_step) + '.pdparams')\n",
    "                paddle.save(model.state_dict(), save_path)\n",
    "                tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for step_, batch in enumerate(dev_data_loader, start=1):\n",
    "            query_input_ids, query_token_type_ids, title_input_ids, title_token_type_ids = batch\n",
    "            loss = model(\n",
    "                query_input_ids=query_input_ids,\n",
    "                title_input_ids=title_input_ids,\n",
    "                query_token_type_ids=query_token_type_ids,\n",
    "                title_token_type_ids=title_token_type_ids)\n",
    "            val_loss += loss.item()\n",
    "        model.train()    \n",
    "        \n",
    "        if val_loss < loss_base:                    \n",
    "                loss_base = val_loss\n",
    "                paddle.save(model.state_dict(), \"./國泰/train_3/pick_loss_model.pdparams\")\n",
    "                \n",
    "        \n",
    "        train_curve.append(train_loss)\n",
    "        val_curve.append(val_loss)\n",
    "        print('epoch: ', epoch, 'train_loss: ', round(train_loss/(step+1),5), 'val_loss: ', round(val_loss/(step_+1),5))\n",
    "\n",
    "        \n",
    "    time_end=time.time()\n",
    "    print('totally cost',time_end-time_start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e7b60",
   "metadata": {},
   "source": [
    "# recall Top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9661621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_id2corpus(corpus_file):\n",
    "    id2corpus = {}\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            id2corpus[idx] = line.rstrip()\n",
    "    return id2corpus\n",
    "\n",
    "\n",
    "def gen_text_file(similar_text_pair_file):\n",
    "    text2similar_text = {}\n",
    "    texts = []\n",
    "    with open(similar_text_pair_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            splited_line = line.rstrip().split(\",\")\n",
    "            if len(splited_line) != 2:\n",
    "                continue\n",
    "\n",
    "            text, similar_text = line.rstrip().split(\",\")\n",
    "\n",
    "            if not text or not similar_text:\n",
    "                continue\n",
    "\n",
    "            text2similar_text[text] = similar_text\n",
    "            texts.append({\"text\": text})\n",
    "    return texts, text2similar_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63519afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "from paddlenlp.utils.log import logger\n",
    "\n",
    "'''\n",
    "HNSW（Hierarchical Navigable Small World）是ANN搜索基於圖的算法，\n",
    "我們要做的是把D维空間中所有的向量構建成一張相互聯通的圖，並基於這張圖搜索某個頂點的K個最近鄰\n",
    "向圖中插入新點時。通過隨機存在的一個點出發查找到距離心點最近的m個點，連接新點到最近的m個點\n",
    "'''\n",
    "\n",
    "def build_index(data_loader, model): #在corpus中做編號\n",
    "\n",
    "    index = hnswlib.Index(space='ip', dim=256) # 確認檢索最近鄰需要使用的距離的方式, 'ip':inner product\n",
    "\n",
    "    # ef表示最近鄰動態列表的大小（需要大於查找的topk），M表示每個結點的“友點”數，是平衡時間/準確率的超參數\n",
    "    # max_elements: the maximum number of elements (capacity)\n",
    "    # ef_construction: controls index search speed/build speed tradeoff\n",
    "    # M - is tightly connected with internal dimensionality of the data. Strongly affects memory consumption (~M)\n",
    "    # Higher M leads to higher accuracy/run_time at fixed ef/efConstruction\n",
    "    index.init_index(max_elements=hnsw_max_elements, ef_construction=hnsw_ef, M=hnsw_m)\n",
    "\n",
    "    # higher ef leads to better accuracy, but slower search\n",
    "    index.set_ef(hnsw_ef)\n",
    "\n",
    "    # Set number of threads used during batch search/construction\n",
    "    # By default using all available cores\n",
    "    index.set_num_threads(16)\n",
    "\n",
    "    logger.info(\"start build index..........\")\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    for text_embeddings in model.get_semantic_embedding(data_loader):\n",
    "        all_embeddings.append(text_embeddings.numpy())\n",
    "\n",
    "    all_embeddings = np.concatenate(all_embeddings, axis=0) #shape = [len(corpus), 256]\n",
    "    index.add_items(all_embeddings)\n",
    "\n",
    "    logger.info(\"Total index number:{}\".format(index.get_current_count()))\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15acb5c8",
   "metadata": {},
   "source": [
    "# 獲取corpus的embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec2818f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from ./國泰/train_3/pick_loss_model.pdparams\n"
     ]
    }
   ],
   "source": [
    "# corpus_file = './國泰/corpus.csv' #召回庫\n",
    "corpus_file = './corpus.csv' \n",
    "recall_result_dir = './國泰/recall'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    batch_size = 32\n",
    "    hnsw_m = 100 # Recall number for each query from Ann index\n",
    "    hnsw_ef = 100 # Recall number for each query from Ann index\n",
    "    hnsw_max_elements = 1000000 # Recall number for each query from Ann index\n",
    "    \n",
    "    # Load pretrained semantic model\n",
    "    params_path = './國泰/train_3/pick_loss_model.pdparams'\n",
    "    if params_path and os.path.isfile(params_path):\n",
    "        state_dict = paddle.load(params_path)\n",
    "        model.set_dict(state_dict)\n",
    "        print(\"Loaded parameters from %s\" % params_path)\n",
    "\n",
    "    id2corpus = gen_id2corpus(corpus_file) # id2corpus = 0:text\n",
    "\n",
    "    # conver_example function's input must be dict\n",
    "    corpus_list = [{idx: text} for idx, text in id2corpus.items()]\n",
    "    corpus_ds = MapDataset(corpus_list)\n",
    "    \n",
    "    trans_func = partial(convert_example,\n",
    "                         tokenizer=tokenizer,\n",
    "                         max_seq_length=30)\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype=\"int64\"),  # text_input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype=\"int64\"),  # text_segment\n",
    "    ): [data for data in fn(samples)]\n",
    "    corpus_data_loader = create_dataloader(corpus_ds,\n",
    "                                           mode='predict',\n",
    "                                           batch_size=32,\n",
    "                                           batchify_fn=batchify_fn,\n",
    "                                           trans_fn=trans_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda7c284",
   "metadata": {},
   "source": [
    "# 採用hnswlib（進行ANN索引）對corpus的Embedding建立資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "027d26e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-02 01:59:53,559] [    INFO]\u001b[0m - start build index..........\u001b[0m\n",
      "\u001b[32m[2022-08-02 02:00:02,470] [    INFO]\u001b[0m - Total index number:14969\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "final_index = build_index(corpus_data_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eebd523",
   "metadata": {},
   "source": [
    "# 獲取評估集Query的Embedding並查詢相似結果，召回 Top3 最相似的文本，並把結果生成在recall_result文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "372cf189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  两天为什么还没有接到确认电话 \n",
      "pair:  为什么还没收到电话确认 \n",
      "similarity:  0.741\n",
      "text:  两天为什么还没有接到确认电话 \n",
      "pair:  为什么审核没有通过 \n",
      "similarity:  0.671\n",
      "text:  两天为什么还没有接到确认电话 \n",
      "pair:  为什么没有评估通过 \n",
      "similarity:  0.664\n",
      "text:  为何放款还没到 \n",
      "pair:  为何还没到账 \n",
      "similarity:  0.825\n",
      "text:  为何放款还没到 \n",
      "pair:  为什么还没到账 \n",
      "similarity:  0.747\n",
      "text:  为何放款还没到 \n",
      "pair:  如何还没款 \n",
      "similarity:  0.733\n",
      "text:  一般多久更新额度的 \n",
      "pair:  一般额度最高是多少 \n",
      "similarity:  0.614\n",
      "text:  一般多久更新额度的 \n",
      "pair:  可以减少额度吗 \n",
      "similarity:  0.585\n",
      "text:  一般多久更新额度的 \n",
      "pair:  一般得多长时间 \n",
      "similarity:  0.576\n",
      "text:  你好，怎么更改电话号码 \n",
      "pair:  你好，如何更改电话号码 \n",
      "similarity:  0.637\n",
      "text:  你好，怎么更改电话号码 \n",
      "pair:  何时再打电话确 \n",
      "similarity:  0.63\n",
      "text:  你好，怎么更改电话号码 \n",
      "pair:  怎么更改电话号码 \n",
      "similarity:  0.552\n",
      "text:  我需要微粒贷 \n",
      "pair:  我不需要微粒贷 \n",
      "similarity:  0.834\n",
      "text:  我需要微粒贷 \n",
      "pair:  我要微粒贷 \n",
      "similarity:  0.695\n",
      "text:  我需要微粒贷 \n",
      "pair:  我想要微粒贷 \n",
      "similarity:  0.695\n"
     ]
    }
   ],
   "source": [
    "similar_text_pair_file = './國泰/val_pair.csv'\n",
    "text_list, text2similar_text = gen_text_file(similar_text_pair_file) #text_list=query, text2similar_text=query:title\n",
    "\n",
    "query_ds = MapDataset(text_list)\n",
    "\n",
    "query_data_loader = create_dataloader(query_ds,\n",
    "                                      mode='predict',\n",
    "                                      batch_size=32,\n",
    "                                      batchify_fn=batchify_fn,\n",
    "                                      trans_fn=trans_func)\n",
    "\n",
    "query_embedding = model.get_semantic_embedding(query_data_loader)\n",
    "\n",
    "\n",
    "recall_result_file = './國泰/recall/recall_weightdecay.txt'\n",
    "recall_num = 3\n",
    "with open(recall_result_file, 'w', encoding='utf-8') as f:\n",
    "    for batch_index, batch_query_embedding in enumerate(query_embedding):\n",
    "        recalled_idx, cosine_sims = final_index.knn_query(batch_query_embedding.numpy(), recall_num)\n",
    "\n",
    "        batch_size = len(cosine_sims)\n",
    "\n",
    "        for row_index in range(batch_size):\n",
    "            text_index = batch_size * batch_index + row_index\n",
    "            for idx, doc_idx in enumerate(recalled_idx[row_index]):\n",
    "                f.write(\"{}\\t{}\\t{}\\n\".format(\n",
    "                    text_list[text_index][\"text\"], id2corpus[doc_idx], 1.0 - cosine_sims[row_index][idx]))\n",
    "                if text_index < 5:\n",
    "                    print('text: ', text_list[text_index][\"text\"], '\\npair: ', id2corpus[doc_idx], '\\nsimilarity: ', round(1.0 - cosine_sims[row_index][idx],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fde0c095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input query: 你好，如何更换还款银行卡\n",
      "\n",
      "text:  你好，如何更换还款银行卡\n",
      "\n",
      "\n",
      "pair:  如何更换卡还款 \n",
      "similarity:  0.732\n",
      "\n",
      "\n",
      "pair:  更换卡后，如何还款 \n",
      "similarity:  0.709\n",
      "\n",
      "\n",
      "pair:  如何换卡还款 \n",
      "similarity:  0.694\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Please input query: \")\n",
    "text_list = []\n",
    "text_list.append({\"text\": text})\n",
    "query_ds = MapDataset(text_list)\n",
    "\n",
    "query_data_loader = create_dataloader(query_ds,\n",
    "                                      mode='predict',\n",
    "                                      batch_size=1,\n",
    "                                      batchify_fn=batchify_fn,\n",
    "                                      trans_fn=trans_func)\n",
    "\n",
    "query_embedding = model.get_semantic_embedding(query_data_loader)\n",
    "\n",
    "\n",
    "recall_result_file = './國泰/recall/try.txt'\n",
    "recall_num = 3\n",
    "with open(recall_result_file, 'w', encoding='utf-8') as f:\n",
    "    for batch_index, batch_query_embedding in enumerate(query_embedding):\n",
    "        recalled_idx, cosine_sims = final_index.knn_query(batch_query_embedding.numpy(), recall_num)\n",
    "\n",
    "        batch_size = len(cosine_sims)\n",
    "\n",
    "        for row_index in range(batch_size):\n",
    "            text_index = batch_size * batch_index + row_index\n",
    "            print('\\ntext: ', text_list[text_index][\"text\"])\n",
    "            for idx, doc_idx in enumerate(recalled_idx[row_index]):\n",
    "                f.write(\"{}\\t{}\\t{}\\n\".format(\n",
    "                    text_list[text_index][\"text\"], id2corpus[doc_idx], 1.0 - cosine_sims[row_index][idx]))\n",
    "                print('\\n\\npair: ', id2corpus[doc_idx], '\\nsimilarity: ', round(1.0 - cosine_sims[row_index][idx],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68a634",
   "metadata": {},
   "source": [
    "# Count recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67687067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@3=1.368\n"
     ]
    }
   ],
   "source": [
    "def recall(rs, N=3):\n",
    "    \"\"\"\n",
    "    Ratio of recalled Ground Truth at topN Recalled Docs\n",
    "    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "    >>> recall(rs, N=1)\n",
    "    0.333333\n",
    "    >>> recall(rs, N=2)\n",
    "    >>> 0.6666667\n",
    "    >>> recall(rs, N=3)\n",
    "    >>> 1.0\n",
    "    Returns:\n",
    "        Recall@N\n",
    "    \"\"\"\n",
    "\n",
    "    recall_flags = [np.sum(r[0:N]) for r in rs]\n",
    "    return np.mean(recall_flags)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    text2similar = {}\n",
    "    with open(similar_text_pair_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if len(line.rstrip().split(\",\")) != 2:\n",
    "                continue\n",
    "            text, similar_text = line.rstrip().split(\",\")\n",
    "            text2similar[text] = similar_text\n",
    "    rs = []\n",
    "    recall_num = 3\n",
    "    recall_result_file = './國泰/recall/recall_weightdecay.txt'\n",
    "    with open(recall_result_file, 'r', encoding='utf-8') as f:\n",
    "        relevance_labels = []\n",
    "        for index, line in enumerate(f):\n",
    "            if index % recall_num == 0 and index != 0:\n",
    "                rs.append(relevance_labels)\n",
    "                relevance_labels = []\n",
    "#             if len(line.rstrip().split(\"\\t\"))!= 3:\n",
    "#                 continue\n",
    "            text, recalled_text, cosine_sim = line.rstrip().split(\"\\t\")\n",
    "            if text2similar[text] == recalled_text:\n",
    "                relevance_labels.append(1)\n",
    "            else:\n",
    "                relevance_labels.append(0)\n",
    "    recall_N = []\n",
    "    recall_num = [3]\n",
    "    result = open('result.tsv', 'a')\n",
    "    res = []\n",
    "    for topN in recall_num:\n",
    "        R = round(100 * recall(rs, N=topN), 3)\n",
    "        recall_N.append(str(R))\n",
    "    for key, val in zip(recall_num, recall_N):\n",
    "        print('recall@{}={}'.format(key, val))\n",
    "        res.append(str(val))\n",
    "    result.write('\\t'.join(res) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5e487",
   "metadata": {},
   "source": [
    "# Customized input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9038eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_result(query1, query2):\n",
    "\n",
    "    trans_func = partial(convert_example, tokenizer=tokenizer, max_seq_length=30)\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # query_input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # query_segment\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # title_input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # tilte_segment\n",
    "    ): [data for data in fn(samples)]        \n",
    "    pair = [list(query1)+list(query2)]\n",
    "    data = pair  \n",
    "\n",
    "    examples = []    \n",
    "    for idx, text in enumerate(data):\n",
    "        input_ids, segment_ids = convert_example({idx: text[0]}, tokenizer)\n",
    "        title_ids, title_segment_ids = convert_example({idx: text[1]}, tokenizer)\n",
    "        examples.append((input_ids, segment_ids, title_ids, title_segment_ids))\n",
    "        \n",
    "    query_ids, query_segment_ids, title_ids, title_segment_ids = batchify_fn(examples)\n",
    "\n",
    "    scale = 20 \n",
    "    output_emb_size = 256 # Output_embedding_size, 0 means use hidden_size as output embedding size\n",
    "    model = SimCSE(pretrained_model,\n",
    "                   scale=scale,\n",
    "                   output_emb_size=output_emb_size)\n",
    "\n",
    "    params_path = './國泰/train_3/pick_loss_model.pdparams'\n",
    "    if params_path and os.path.isfile(params_path):\n",
    "        state_dict = paddle.load(params_path)\n",
    "        model.set_dict(state_dict)\n",
    "        print(\"Loaded parameters from %s\" % params_path)\n",
    "\n",
    "    model.eval()\n",
    "    cosine_sims = []\n",
    "    with paddle.no_grad():\n",
    "        batch_cosine_sim = model.cosine_sim(\n",
    "        query_input_ids=query_ids,\n",
    "        title_input_ids=title_ids,\n",
    "        query_token_type_ids=query_segment_ids,\n",
    "        title_token_type_ids=title_segment_ids).numpy()\n",
    "        cosine_sims.append(batch_cosine_sim)\n",
    "\n",
    "    cosine_sims = np.concatenate(cosine_sims, axis=0)\n",
    "    for idx, cosine in enumerate(cosine_sims):\n",
    "        print('{}'.format(cosine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b83820c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input query1微粒贷不见了\n",
      "Please input query2为什么我的微粒贷突然不见了\n",
      "Loaded parameters from ./國泰/train_3/pick_loss_model.pdparams\n",
      "0.735389769077301\n"
     ]
    }
   ],
   "source": [
    "query1 = input(\"Please input query1\")\n",
    "query2 = input(\"Please input query2\")\n",
    "test_result(query1, query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "660bb628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input query1为什么会有短信说审核不通过\n",
      "Please input query2为什么借款回复是未通过了？\n",
      "Loaded parameters from ./國泰/train_3/pick_loss_model.pdparams\n",
      "0.6633898615837097\n"
     ]
    }
   ],
   "source": [
    "query1 = input(\"Please input query1\")\n",
    "query2 = input(\"Please input query2\")\n",
    "test_result(query1, query2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
